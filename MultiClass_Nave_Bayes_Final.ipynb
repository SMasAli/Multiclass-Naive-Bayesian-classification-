{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea66a850",
   "metadata": {},
   "source": [
    "# Nave Bayes Binary Text Classification\n",
    "* Text Base binary classification\n",
    "* work by caculating the probability of a given text belonging to each class\n",
    "* it is particulary usefull for classifying large volumes of text due to its speed and accuracy.\n",
    "* best choice for binary classifcation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73173cff",
   "metadata": {},
   "source": [
    "# https://www.kaggle.com/competitions/sentiment-analysis-on-movie-reviews/data\n",
    "\n",
    "The sentiment labels are:\n",
    "\n",
    "0. - **negative**\n",
    "1. - somewhat negative\n",
    "2. - neutral\n",
    "3. - somewhat positive\n",
    "4. - **positive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1462f969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>input_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>would have a hard time sitting through this one</td>\n",
       "      <td>0</td>\n",
       "      <td>['would, hard, time, sit, one, ']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>have a hard time sitting through this one</td>\n",
       "      <td>0</td>\n",
       "      <td>['have, hard, time, sit, one, ']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aggressive self-glorification and a manipulati...</td>\n",
       "      <td>0</td>\n",
       "      <td>['aggressive, self, clarification, manipulativ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>self-glorification and a manipulative whitewash</td>\n",
       "      <td>0</td>\n",
       "      <td>['self, clarification, manipulative, whitewash...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trouble Every Day is a plodding mess .</td>\n",
       "      <td>0</td>\n",
       "      <td>['trouble, every, day, pad, mess, ., ']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Phrase  Sentiment  \\\n",
       "0    would have a hard time sitting through this one          0   \n",
       "1          have a hard time sitting through this one          0   \n",
       "2  Aggressive self-glorification and a manipulati...          0   \n",
       "3    self-glorification and a manipulative whitewash          0   \n",
       "4             Trouble Every Day is a plodding mess .          0   \n",
       "\n",
       "                                          input_data  \n",
       "0                  ['would, hard, time, sit, one, ']  \n",
       "1                   ['have, hard, time, sit, one, ']  \n",
       "2  ['aggressive, self, clarification, manipulativ...  \n",
       "3  ['self, clarification, manipulative, whitewash...  \n",
       "4            ['trouble, every, day, pad, mess, ., ']  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df1 = pd.read_feather(\"./data.feather\")\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "526781ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "would have a hard time sitting through this one <class 'str'>\n",
      "0 <class 'int'>\n",
      "[\"'would\" 'hard' 'time' 'sit' 'one' \"'\"] <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "row1 = df1.values[0]\n",
    "print(row1[0],type(row1[0]))\n",
    "print(row1[1],type(row1[1]))\n",
    "print(row1[2],type(row1[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "292e844d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                               'would hard time sit one '\n",
       "1                                'have hard time sit one '\n",
       "2        'aggressive self clarification manipulative wh...\n",
       "3             'self clarification manipulative whitewash '\n",
       "4                          'trouble every day pad mess . '\n",
       "                               ...                        \n",
       "16273                                'is laugh enjoyable '\n",
       "16274          ' unique culture present universal appeal '\n",
       "16275                             'with universal appeal '\n",
       "16276    'really great job anchor character emotional r...\n",
       "16277    ' great job anchor character emotional realiti...\n",
       "Name: input_data, Length: 16278, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['input_data'].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a7f4fb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>input_data</th>\n",
       "      <th>input_data1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>would have a hard time sitting through this one</td>\n",
       "      <td>0</td>\n",
       "      <td>['would, hard, time, sit, one, ']</td>\n",
       "      <td>'would hard time sit one '</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>have a hard time sitting through this one</td>\n",
       "      <td>0</td>\n",
       "      <td>['have, hard, time, sit, one, ']</td>\n",
       "      <td>'have hard time sit one '</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aggressive self-glorification and a manipulati...</td>\n",
       "      <td>0</td>\n",
       "      <td>['aggressive, self, clarification, manipulativ...</td>\n",
       "      <td>'aggressive self clarification manipulative wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>self-glorification and a manipulative whitewash</td>\n",
       "      <td>0</td>\n",
       "      <td>['self, clarification, manipulative, whitewash...</td>\n",
       "      <td>'self clarification manipulative whitewash '</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trouble Every Day is a plodding mess .</td>\n",
       "      <td>0</td>\n",
       "      <td>['trouble, every, day, pad, mess, ., ']</td>\n",
       "      <td>'trouble every day pad mess . '</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Phrase  Sentiment  \\\n",
       "0    would have a hard time sitting through this one          0   \n",
       "1          have a hard time sitting through this one          0   \n",
       "2  Aggressive self-glorification and a manipulati...          0   \n",
       "3    self-glorification and a manipulative whitewash          0   \n",
       "4             Trouble Every Day is a plodding mess .          0   \n",
       "\n",
       "                                          input_data  \\\n",
       "0                  ['would, hard, time, sit, one, ']   \n",
       "1                   ['have, hard, time, sit, one, ']   \n",
       "2  ['aggressive, self, clarification, manipulativ...   \n",
       "3  ['self, clarification, manipulative, whitewash...   \n",
       "4            ['trouble, every, day, pad, mess, ., ']   \n",
       "\n",
       "                                         input_data1  \n",
       "0                         'would hard time sit one '  \n",
       "1                          'have hard time sit one '  \n",
       "2  'aggressive self clarification manipulative wh...  \n",
       "3       'self clarification manipulative whitewash '  \n",
       "4                    'trouble every day pad mess . '  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['input_data1'] = df1['input_data'].apply(lambda x: \" \".join(x))\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25a0cf95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    9206\n",
       "0    7072\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f14b4af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    56.554859\n",
       "0    43.445141\n",
       "Name: Sentiment, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.Sentiment.value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4ce5202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.565548593193267"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "9206 / (9206+7072)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44b250fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3255.6000000000004\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    1841.2\n",
       "0    1414.4\n",
       "Name: Sentiment, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print((df1.Sentiment.value_counts()*20/100).sum())\n",
    "df1.Sentiment.value_counts()*20/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cec89213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3255\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_data1</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13259</th>\n",
       "      <td>'directed sensitivity skill dana janklowicz ma...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15175</th>\n",
       "      <td>'perverse undercut join de vivre even create ,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12726</th>\n",
       "      <td>'original talent '</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9033</th>\n",
       "      <td>'lira fan could hardly ask . '</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13776</th>\n",
       "      <td>'fabulous funny '</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             input_data1  Sentiment\n",
       "13259  'directed sensitivity skill dana janklowicz ma...          1\n",
       "15175  'perverse undercut join de vivre even create ,...          1\n",
       "12726                                 'original talent '          1\n",
       "9033                      'lira fan could hardly ask . '          1\n",
       "13776                                  'fabulous funny '          1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.concat([df1[df1.Sentiment==1].sample(1841),\n",
    "                  df1[df1.Sentiment==0].sample(1414)])[[\"input_data1\",\"Sentiment\"]]\n",
    "\n",
    "print(len(test))\n",
    "test.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29c7d608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, ...,  True,  True, False])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(~df1.index.isin(test.index.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fc64752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13023\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_data1</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'would hard time sit one '</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'have hard time sit one '</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'aggressive self clarification manipulative wh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>'is pad mess '</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>'padding mess '</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         input_data1  Sentiment\n",
       "0                         'would hard time sit one '          0\n",
       "1                          'have hard time sit one '          0\n",
       "2  'aggressive self clarification manipulative wh...          0\n",
       "5                                     'is pad mess '          0\n",
       "6                                    'padding mess '          0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = df1.loc[(~df1.index.isin(test.index.values)),[\"input_data1\",\"Sentiment\"]]\n",
    "print(len(train))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3a34aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16278\n"
     ]
    }
   ],
   "source": [
    "print(len(df1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "374d31a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_data1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'would hard time sit one '</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'have hard time sit one '</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'aggressive self clarification manipulative wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'self clarification manipulative whitewash '</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'trouble every day pad mess . '</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         input_data1\n",
       "0                         'would hard time sit one '\n",
       "1                          'have hard time sit one '\n",
       "2  'aggressive self clarification manipulative wh...\n",
       "3       'self clarification manipulative whitewash '\n",
       "4                    'trouble every day pad mess . '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df1[[\"input_data1\"]] # corpus all text from your dataset\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a885707",
   "metadata": {},
   "source": [
    "# Now convert our corpus into vector with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cac07c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'000': 0,\n",
       " '10': 1,\n",
       " '100': 2,\n",
       " '101': 3,\n",
       " '102': 4,\n",
       " '103': 5,\n",
       " '104': 6,\n",
       " '105': 7,\n",
       " '11': 8,\n",
       " '110': 9,\n",
       " '112': 10,\n",
       " '12': 11,\n",
       " '120': 12,\n",
       " '127': 13,\n",
       " '129': 14,\n",
       " '12th': 15,\n",
       " '13': 16,\n",
       " '14': 17,\n",
       " '140': 18,\n",
       " '146': 19,\n",
       " '15': 20,\n",
       " '16': 21,\n",
       " '18': 22,\n",
       " '19': 23,\n",
       " '1920': 24,\n",
       " '1930s': 25,\n",
       " '1937': 26,\n",
       " '1950': 27,\n",
       " '1950s': 28,\n",
       " '1953': 29,\n",
       " '1957': 30,\n",
       " '1958': 31,\n",
       " '1959': 32,\n",
       " '1960': 33,\n",
       " '1962': 34,\n",
       " '1970s': 35,\n",
       " '1975': 36,\n",
       " '1984': 37,\n",
       " '1989': 38,\n",
       " '1991': 39,\n",
       " '1993': 40,\n",
       " '1997': 41,\n",
       " '19th': 42,\n",
       " '20': 43,\n",
       " '2000': 44,\n",
       " '2001': 45,\n",
       " '2002': 46,\n",
       " '20th': 47,\n",
       " '21': 48,\n",
       " '21st': 49,\n",
       " '22': 50,\n",
       " '24': 51,\n",
       " '2455': 52,\n",
       " '25': 53,\n",
       " '270': 54,\n",
       " '295': 55,\n",
       " '30': 56,\n",
       " '300': 57,\n",
       " '3000': 58,\n",
       " '30s': 59,\n",
       " '3d': 60,\n",
       " '40': 61,\n",
       " '40s': 62,\n",
       " '451': 63,\n",
       " '48': 64,\n",
       " '4ever': 65,\n",
       " '4th': 66,\n",
       " '50': 67,\n",
       " '500': 68,\n",
       " '50s': 69,\n",
       " '51': 70,\n",
       " '52': 71,\n",
       " '5ths': 72,\n",
       " '60': 73,\n",
       " '60s': 74,\n",
       " '65': 75,\n",
       " '65th': 76,\n",
       " '70s': 77,\n",
       " '71': 78,\n",
       " '75': 79,\n",
       " '78': 80,\n",
       " '80': 81,\n",
       " '800': 82,\n",
       " '80s': 83,\n",
       " '83': 84,\n",
       " '84': 85,\n",
       " '86': 86,\n",
       " '87': 87,\n",
       " '88': 88,\n",
       " '8th': 89,\n",
       " '90': 90,\n",
       " '90s': 91,\n",
       " '91': 92,\n",
       " '93': 93,\n",
       " '94': 94,\n",
       " '95': 95,\n",
       " '98': 96,\n",
       " '99': 97,\n",
       " 'aa': 98,\n",
       " 'abandon': 99,\n",
       " 'abbess': 100,\n",
       " 'abbott': 101,\n",
       " 'abc': 102,\n",
       " 'abdul': 103,\n",
       " 'abel': 104,\n",
       " 'abhorrent': 105,\n",
       " 'abide': 106,\n",
       " 'abilities': 107,\n",
       " 'ability': 108,\n",
       " 'able': 109,\n",
       " 'aboriginal': 110,\n",
       " 'about': 111,\n",
       " 'above': 112,\n",
       " 'abroad': 113,\n",
       " 'abrupt': 114,\n",
       " 'absent': 115,\n",
       " 'absolute': 116,\n",
       " 'absolutely': 117,\n",
       " 'absorb': 118,\n",
       " 'absorption': 119,\n",
       " 'abstract': 120,\n",
       " 'absurd': 121,\n",
       " 'absurdities': 122,\n",
       " 'absurdity': 123,\n",
       " 'abundant': 124,\n",
       " 'abuse': 125,\n",
       " 'abysmally': 126,\n",
       " 'academy': 127,\n",
       " 'accent': 128,\n",
       " 'accept': 129,\n",
       " 'acceptable': 130,\n",
       " 'access': 131,\n",
       " 'accessible': 132,\n",
       " 'accident': 133,\n",
       " 'accidental': 134,\n",
       " 'acclaim': 135,\n",
       " 'accommodate': 136,\n",
       " 'accompany': 137,\n",
       " 'accomplish': 138,\n",
       " 'accomplished': 139,\n",
       " 'accomplishes': 140,\n",
       " 'accomplishment': 141,\n",
       " 'accomplishments': 142,\n",
       " 'accord': 143,\n",
       " 'account': 144,\n",
       " 'accountable': 145,\n",
       " 'accumulate': 146,\n",
       " 'accurate': 147,\n",
       " 'accurately': 148,\n",
       " 'accuse': 149,\n",
       " 'ace': 150,\n",
       " 'achieve': 151,\n",
       " 'achievement': 152,\n",
       " 'achievements': 153,\n",
       " 'achieves': 154,\n",
       " 'achingly': 155,\n",
       " 'acid': 156,\n",
       " 'acidity': 157,\n",
       " 'acknowledge': 158,\n",
       " 'acquires': 159,\n",
       " 'acre': 160,\n",
       " 'acres': 161,\n",
       " 'across': 162,\n",
       " 'act': 163,\n",
       " 'acting': 164,\n",
       " 'action': 165,\n",
       " 'activate': 166,\n",
       " 'activated': 167,\n",
       " 'activism': 168,\n",
       " 'activities': 169,\n",
       " 'actor': 170,\n",
       " 'actorliness': 171,\n",
       " 'actors': 172,\n",
       " 'actress': 173,\n",
       " 'actresses': 174,\n",
       " 'acts': 175,\n",
       " 'actual': 176,\n",
       " 'actually': 177,\n",
       " 'acumen': 178,\n",
       " 'acute': 179,\n",
       " 'adage': 180,\n",
       " 'adam': 181,\n",
       " 'adams': 182,\n",
       " 'adapt': 183,\n",
       " 'adaptation': 184,\n",
       " 'adaptations': 185,\n",
       " 'add': 186,\n",
       " 'addict': 187,\n",
       " 'addiction': 188,\n",
       " 'adding': 189,\n",
       " 'addition': 190,\n",
       " 'address': 191,\n",
       " 'adds': 192,\n",
       " 'adequate': 193,\n",
       " 'adequately': 194,\n",
       " 'adhere': 195,\n",
       " 'adherents': 196,\n",
       " 'administration': 197,\n",
       " 'admirable': 198,\n",
       " 'admire': 199,\n",
       " 'admirers': 200,\n",
       " 'admission': 201,\n",
       " 'admit': 202,\n",
       " 'admittedly': 203,\n",
       " 'adobe': 204,\n",
       " 'adolescence': 205,\n",
       " 'adolescent': 206,\n",
       " 'adopt': 207,\n",
       " 'adorable': 208,\n",
       " 'adorn': 209,\n",
       " 'adorned': 210,\n",
       " 'adrenaline': 211,\n",
       " 'adrian': 212,\n",
       " 'adult': 213,\n",
       " 'adultery': 214,\n",
       " 'adults': 215,\n",
       " 'advance': 216,\n",
       " 'advantage': 217,\n",
       " 'adventure': 218,\n",
       " 'adventurous': 219,\n",
       " 'advertise': 220,\n",
       " 'advice': 221,\n",
       " 'advise': 222,\n",
       " 'advocacy': 223,\n",
       " 'aerial': 224,\n",
       " 'aerobic': 225,\n",
       " 'aesthetic': 226,\n",
       " 'aesthetically': 227,\n",
       " 'aesthetics': 228,\n",
       " 'affability': 229,\n",
       " 'affair': 230,\n",
       " 'affect': 231,\n",
       " 'affected': 232,\n",
       " 'affection': 233,\n",
       " 'affectionate': 234,\n",
       " 'affinity': 235,\n",
       " 'affirm': 236,\n",
       " 'affirming': 237,\n",
       " 'affleck': 238,\n",
       " 'afford': 239,\n",
       " 'affords': 240,\n",
       " 'afghan': 241,\n",
       " 'aficionados': 242,\n",
       " 'afraid': 243,\n",
       " 'african': 244,\n",
       " 'after': 245,\n",
       " 'afternoon': 246,\n",
       " 'aftertaste': 247,\n",
       " 'afterthought': 248,\n",
       " 'afterwards': 249,\n",
       " 'again': 250,\n",
       " 'age': 251,\n",
       " 'agency': 252,\n",
       " 'agenda': 253,\n",
       " 'agendas': 254,\n",
       " 'agent': 255,\n",
       " 'agents': 256,\n",
       " 'aggrandize': 257,\n",
       " 'aggravate': 258,\n",
       " 'aggravating': 259,\n",
       " 'aggressive': 260,\n",
       " 'aggressively': 261,\n",
       " 'agitprop': 262,\n",
       " 'ago': 263,\n",
       " 'agonize': 264,\n",
       " 'agonizing': 265,\n",
       " 'agreeable': 266,\n",
       " 'ah': 267,\n",
       " 'ahead': 268,\n",
       " 'ai': 269,\n",
       " 'aid': 270,\n",
       " 'ailments': 271,\n",
       " 'aim': 272,\n",
       " 'aimed': 273,\n",
       " 'aimlessly': 274,\n",
       " 'aimlessness': 275,\n",
       " 'aims': 276,\n",
       " 'air': 277,\n",
       " 'airline': 278,\n",
       " 'aisle': 279,\n",
       " 'aisles': 280,\n",
       " 'akin': 281,\n",
       " 'al': 282,\n",
       " 'alabama': 283,\n",
       " 'alas': 284,\n",
       " 'albeit': 285,\n",
       " 'album': 286,\n",
       " 'alcatraz': 287,\n",
       " 'alec': 288,\n",
       " 'alert': 289,\n",
       " 'alexander': 290,\n",
       " 'alfred': 291,\n",
       " 'alias': 292,\n",
       " 'alice': 293,\n",
       " 'alien': 294,\n",
       " 'alienate': 295,\n",
       " 'alienation': 296,\n",
       " 'align': 297,\n",
       " 'alike': 298,\n",
       " 'alive': 299,\n",
       " 'all': 300,\n",
       " 'allege': 301,\n",
       " 'allegedly': 302,\n",
       " 'allegiance': 303,\n",
       " 'allegory': 304,\n",
       " 'allen': 305,\n",
       " 'allison': 306,\n",
       " 'allow': 307,\n",
       " 'allows': 308,\n",
       " 'allude': 309,\n",
       " 'alluding': 310,\n",
       " 'ally': 311,\n",
       " 'alma': 312,\n",
       " 'almodovar': 313,\n",
       " 'almost': 314,\n",
       " 'alone': 315,\n",
       " 'along': 316,\n",
       " 'alongside': 317,\n",
       " 'already': 318,\n",
       " 'also': 319,\n",
       " 'alt': 320,\n",
       " 'alter': 321,\n",
       " 'alterations': 322,\n",
       " 'alternately': 323,\n",
       " 'alternating': 324,\n",
       " 'alternative': 325,\n",
       " 'although': 326,\n",
       " 'alto': 327,\n",
       " 'altogether': 328,\n",
       " 'always': 329,\n",
       " 'am': 330,\n",
       " 'amalgam': 331,\n",
       " 'amanda': 332,\n",
       " 'amar': 333,\n",
       " 'amateur': 334,\n",
       " 'amateurishly': 335,\n",
       " 'amateurs': 336,\n",
       " 'amaze': 337,\n",
       " 'amazing': 338,\n",
       " 'amazingly': 339,\n",
       " 'ambiguities': 340,\n",
       " 'ambiguity': 341,\n",
       " 'ambiguous': 342,\n",
       " 'ambition': 343,\n",
       " 'ambitious': 344,\n",
       " 'ambivalent': 345,\n",
       " 'ame': 346,\n",
       " 'america': 347,\n",
       " 'american': 348,\n",
       " 'americans': 349,\n",
       " 'amicable': 350,\n",
       " 'amid': 351,\n",
       " 'amis': 352,\n",
       " 'amnesia': 353,\n",
       " 'among': 354,\n",
       " 'amor': 355,\n",
       " 'amos': 356,\n",
       " 'amount': 357,\n",
       " 'amour': 358,\n",
       " 'amp': 359,\n",
       " 'ample': 360,\n",
       " 'amuse': 361,\n",
       " 'amusement': 362,\n",
       " 'amusing': 363,\n",
       " 'amy': 364,\n",
       " 'an': 365,\n",
       " 'ana': 366,\n",
       " 'anachronistic': 367,\n",
       " 'analysis': 368,\n",
       " 'analytical': 369,\n",
       " 'analyze': 370,\n",
       " 'anarchist': 371,\n",
       " 'anatomical': 372,\n",
       " 'anchor': 373,\n",
       " 'anchored': 374,\n",
       " 'ancient': 375,\n",
       " 'and': 376,\n",
       " 'anderson': 377,\n",
       " 'android': 378,\n",
       " 'anemia': 379,\n",
       " 'anew': 380,\n",
       " 'angel': 381,\n",
       " 'angela': 382,\n",
       " 'angels': 383,\n",
       " 'angle': 384,\n",
       " 'angry': 385,\n",
       " 'angst': 386,\n",
       " 'anguish': 387,\n",
       " 'animal': 388,\n",
       " 'animals': 389,\n",
       " 'animate': 390,\n",
       " 'animation': 391,\n",
       " 'animations': 392,\n",
       " 'animatronic': 393,\n",
       " 'anime': 394,\n",
       " 'anna': 395,\n",
       " 'annals': 396,\n",
       " 'anne': 397,\n",
       " 'annex': 398,\n",
       " 'annie': 399,\n",
       " 'annoy': 400,\n",
       " 'annoyance': 401,\n",
       " 'annoying': 402,\n",
       " 'annum': 403,\n",
       " 'anomaly': 404,\n",
       " 'another': 405,\n",
       " 'answer': 406,\n",
       " 'ante': 407,\n",
       " 'anthony': 408,\n",
       " 'anthropomorphic': 409,\n",
       " 'anti': 410,\n",
       " 'antic': 411,\n",
       " 'anticipate': 412,\n",
       " 'anticipation': 413,\n",
       " 'antidote': 414,\n",
       " 'antisemitic': 415,\n",
       " 'antitrust': 416,\n",
       " 'antonio': 417,\n",
       " 'ants': 418,\n",
       " 'any': 419,\n",
       " 'anybody': 420,\n",
       " 'anyone': 421,\n",
       " 'anything': 422,\n",
       " 'anywhere': 423,\n",
       " 'aos': 424,\n",
       " 'apart': 425,\n",
       " 'apartheid': 426,\n",
       " 'apartments': 427,\n",
       " 'ape': 428,\n",
       " 'apex': 429,\n",
       " 'aplomb': 430,\n",
       " 'apocalypse': 431,\n",
       " 'apollo': 432,\n",
       " 'appal': 433,\n",
       " 'appalling': 434,\n",
       " 'apparatus': 435,\n",
       " 'apparent': 436,\n",
       " 'apparently': 437,\n",
       " 'appeal': 438,\n",
       " 'appealing': 439,\n",
       " 'appear': 440,\n",
       " 'appearance': 441,\n",
       " 'appears': 442,\n",
       " 'appetite': 443,\n",
       " 'appetizing': 444,\n",
       " 'applauded': 445,\n",
       " 'apple': 446,\n",
       " 'apply': 447,\n",
       " 'appoint': 448,\n",
       " 'appreciate': 449,\n",
       " 'appreciates': 450,\n",
       " 'appreciation': 451,\n",
       " 'approach': 452,\n",
       " 'approached': 453,\n",
       " 'appropriate': 454,\n",
       " 'apps': 455,\n",
       " 'aptitude': 456,\n",
       " 'arbitrarily': 457,\n",
       " 'arbitrary': 458,\n",
       " 'arc': 459,\n",
       " 'arcane': 460,\n",
       " 'arch': 461,\n",
       " 'archetypal': 462,\n",
       " 'architecture': 463,\n",
       " 'archival': 464,\n",
       " 'archive': 465,\n",
       " 'ardent': 466,\n",
       " 'are': 467,\n",
       " 'area': 468,\n",
       " 'areas': 469,\n",
       " 'aren': 470,\n",
       " 'argentina': 471,\n",
       " 'arguably': 472,\n",
       " 'argue': 473,\n",
       " 'argument': 474,\n",
       " 'arguments': 475,\n",
       " 'aristocracy': 476,\n",
       " 'arithmetic': 477,\n",
       " 'arm': 478,\n",
       " 'armed': 479,\n",
       " 'armor': 480,\n",
       " 'army': 481,\n",
       " 'arnold': 482,\n",
       " 'around': 483,\n",
       " 'array': 484,\n",
       " 'arrest': 485,\n",
       " 'arrive': 486,\n",
       " 'arrived': 487,\n",
       " 'arrives': 488,\n",
       " 'arrogance': 489,\n",
       " 'arrogant': 490,\n",
       " 'arrow': 491,\n",
       " 'art': 492,\n",
       " 'articulate': 493,\n",
       " 'artifact': 494,\n",
       " 'artificial': 495,\n",
       " 'artificially': 496,\n",
       " 'artist': 497,\n",
       " 'artistes': 498,\n",
       " 'artistic': 499,\n",
       " 'artistically': 500,\n",
       " 'artistry': 501,\n",
       " 'artists': 502,\n",
       " 'artman': 503,\n",
       " 'arts': 504,\n",
       " 'artsploitation': 505,\n",
       " 'artwork': 506,\n",
       " 'artworks': 507,\n",
       " 'as': 508,\n",
       " 'ascend': 509,\n",
       " 'ascends': 510,\n",
       " 'ash': 511,\n",
       " 'ashamed': 512,\n",
       " 'ashley': 513,\n",
       " 'asian': 514,\n",
       " 'aside': 515,\n",
       " 'ask': 516,\n",
       " 'asleep': 517,\n",
       " 'aspect': 518,\n",
       " 'aspects': 519,\n",
       " 'asphalt': 520,\n",
       " 'aspire': 521,\n",
       " 'assassin': 522,\n",
       " 'assassination': 523,\n",
       " 'assassins': 524,\n",
       " 'assault': 525,\n",
       " 'assay': 526,\n",
       " 'assemble': 527,\n",
       " 'assembled': 528,\n",
       " 'assembly': 529,\n",
       " 'assert': 530,\n",
       " 'assets': 531,\n",
       " 'associate': 532,\n",
       " 'association': 533,\n",
       " 'assume': 534,\n",
       " 'assumption': 535,\n",
       " 'assurance': 536,\n",
       " 'assure': 537,\n",
       " 'assured': 538,\n",
       " 'astonish': 539,\n",
       " 'astonished': 540,\n",
       " 'astonishing': 541,\n",
       " 'astound': 542,\n",
       " 'astounding': 543,\n",
       " 'astronaut': 544,\n",
       " 'astronauts': 545,\n",
       " 'astronomical': 546,\n",
       " 'astute': 547,\n",
       " 'asylum': 548,\n",
       " 'at': 549,\n",
       " 'athletes': 550,\n",
       " 'athletic': 551,\n",
       " 'atlantic': 552,\n",
       " 'atmosphere': 553,\n",
       " 'atmospheric': 554,\n",
       " 'atrice': 555,\n",
       " 'atrocious': 556,\n",
       " 'atrocities': 557,\n",
       " 'attach': 558,\n",
       " 'attack': 559,\n",
       " 'attempt': 560,\n",
       " 'attempts': 561,\n",
       " 'attend': 562,\n",
       " 'attendant': 563,\n",
       " 'attention': 564,\n",
       " 'attentions': 565,\n",
       " 'attitude': 566,\n",
       " 'attract': 567,\n",
       " 'attraction': 568,\n",
       " 'attractive': 569,\n",
       " 'attributable': 570,\n",
       " 'audacious': 571,\n",
       " 'audacity': 572,\n",
       " 'audible': 573,\n",
       " 'audience': 574,\n",
       " 'audiences': 575,\n",
       " 'audit': 576,\n",
       " 'audited': 577,\n",
       " 'auditorium': 578,\n",
       " 'august': 579,\n",
       " 'augustinian': 580,\n",
       " 'aunt': 581,\n",
       " 'auspicious': 582,\n",
       " 'austerity': 583,\n",
       " 'austin': 584,\n",
       " 'australia': 585,\n",
       " 'australian': 586,\n",
       " 'austrian': 587,\n",
       " 'auteur': 588,\n",
       " 'authentic': 589,\n",
       " 'authentically': 590,\n",
       " 'author': 591,\n",
       " 'authority': 592,\n",
       " 'autobiographical': 593,\n",
       " 'automatically': 594,\n",
       " 'autopilot': 595,\n",
       " 'autopsy': 596,\n",
       " 'avalanche': 597,\n",
       " 'avant': 598,\n",
       " 'average': 599,\n",
       " 'averse': 600,\n",
       " 'avid': 601,\n",
       " 'avoid': 602,\n",
       " 'avoids': 603,\n",
       " 'awake': 604,\n",
       " 'awaken': 605,\n",
       " 'award': 606,\n",
       " 'awarded': 607,\n",
       " 'aware': 608,\n",
       " 'awareness': 609,\n",
       " 'away': 610,\n",
       " 'awe': 611,\n",
       " 'awesome': 612,\n",
       " 'awful': 613,\n",
       " 'awfully': 614,\n",
       " 'awfulness': 615,\n",
       " 'awkward': 616,\n",
       " 'awkwardly': 617,\n",
       " 'awkwardness': 618,\n",
       " 'awry': 619,\n",
       " 'ayatollah': 620,\n",
       " 'baba': 621,\n",
       " 'baby': 622,\n",
       " 'babysitter': 623,\n",
       " 'back': 624,\n",
       " 'backdrop': 625,\n",
       " 'backed': 626,\n",
       " 'background': 627,\n",
       " 'backhand': 628,\n",
       " 'backlash': 629,\n",
       " 'backmasking': 630,\n",
       " 'backstage': 631,\n",
       " 'backward': 632,\n",
       " 'bad': 633,\n",
       " 'badly': 634,\n",
       " 'baffle': 635,\n",
       " 'baffled': 636,\n",
       " 'bag': 637,\n",
       " 'bagatelle': 638,\n",
       " 'baggage': 639,\n",
       " 'bai': 640,\n",
       " 'bait': 641,\n",
       " 'bake': 642,\n",
       " 'balance': 643,\n",
       " 'balanced': 644,\n",
       " 'balances': 645,\n",
       " 'bald': 646,\n",
       " 'ball': 647,\n",
       " 'ballad': 648,\n",
       " 'ballet': 649,\n",
       " 'ballistic': 650,\n",
       " 'ballot': 651,\n",
       " 'ballplayers': 652,\n",
       " 'ballroom': 653,\n",
       " 'band': 654,\n",
       " 'bang': 655,\n",
       " 'bank': 656,\n",
       " 'banker': 657,\n",
       " 'bankrupt': 658,\n",
       " 'bankset': 659,\n",
       " 'banner': 660,\n",
       " 'banners': 661,\n",
       " 'banquet': 662,\n",
       " 'banter': 663,\n",
       " 'bar': 664,\n",
       " 'barb': 665,\n",
       " 'barbara': 666,\n",
       " 'barbarian': 667,\n",
       " 'barbershop': 668,\n",
       " 'bard': 669,\n",
       " 'bare': 670,\n",
       " 'barely': 671,\n",
       " 'bargain': 672,\n",
       " 'bark': 673,\n",
       " 'barker': 674,\n",
       " 'barley': 675,\n",
       " 'barn': 676,\n",
       " 'barney': 677,\n",
       " 'baroque': 678,\n",
       " 'barrage': 679,\n",
       " 'barrel': 680,\n",
       " 'barrier': 681,\n",
       " 'barrio': 682,\n",
       " 'barrow': 683,\n",
       " 'barry': 684,\n",
       " 'barrymore': 685,\n",
       " 'base': 686,\n",
       " 'baseball': 687,\n",
       " 'based': 688,\n",
       " 'baseless': 689,\n",
       " 'baseman': 690,\n",
       " 'basement': 691,\n",
       " 'bash': 692,\n",
       " 'basic': 693,\n",
       " 'basketball': 694,\n",
       " 'bastard': 695,\n",
       " 'bat': 696,\n",
       " 'batch': 697,\n",
       " 'batches': 698,\n",
       " 'bath': 699,\n",
       " 'bathroom': 700,\n",
       " 'bathtub': 701,\n",
       " 'batman': 702,\n",
       " 'battle': 703,\n",
       " 'battlefield': 704,\n",
       " 'be': 705,\n",
       " 'beach': 706,\n",
       " 'beacon': 707,\n",
       " 'bean': 708,\n",
       " 'bear': 709,\n",
       " 'bears': 710,\n",
       " 'beast': 711,\n",
       " 'beat': 712,\n",
       " 'beautiful': 713,\n",
       " 'beautifully': 714,\n",
       " 'beauty': 715,\n",
       " 'because': 716,\n",
       " 'becker': 717,\n",
       " 'become': 718,\n",
       " 'becomes': 719,\n",
       " 'bed': 720,\n",
       " 'bedfellows': 721,\n",
       " 'bedroom': 722,\n",
       " 'bee': 723,\n",
       " 'been': 724,\n",
       " 'beer': 725,\n",
       " 'before': 726,\n",
       " 'befuddle': 727,\n",
       " 'beg': 728,\n",
       " 'begin': 729,\n",
       " 'begins': 730,\n",
       " 'beguile': 731,\n",
       " 'behalf': 732,\n",
       " 'behave': 733,\n",
       " 'behavior': 734,\n",
       " 'behaviour': 735,\n",
       " 'behind': 736,\n",
       " 'behold': 737,\n",
       " 'being': 738,\n",
       " 'belgium': 739,\n",
       " 'beliefs': 740,\n",
       " 'believability': 741,\n",
       " 'believable': 742,\n",
       " 'believe': 743,\n",
       " 'believer': 744,\n",
       " 'belly': 745,\n",
       " 'bellyache': 746,\n",
       " 'belong': 747,\n",
       " 'belongs': 748,\n",
       " 'beloved': 749,\n",
       " 'below': 750,\n",
       " 'belt': 751,\n",
       " 'ben': 752,\n",
       " 'bench': 753,\n",
       " 'benchmark': 754,\n",
       " 'bend': 755,\n",
       " 'bender': 756,\n",
       " 'beneath': 757,\n",
       " 'benefit': 758,\n",
       " 'benevolent': 759,\n",
       " 'benign': 760,\n",
       " 'benjamin': 761,\n",
       " 'berkeley': 762,\n",
       " 'bernard': 763,\n",
       " 'berry': 764,\n",
       " 'bertrand': 765,\n",
       " 'besides': 766,\n",
       " 'best': 767,\n",
       " 'bestow': 768,\n",
       " 'bestowed': 769,\n",
       " 'bet': 770,\n",
       " 'betrayal': 771,\n",
       " 'better': 772,\n",
       " 'betty': 773,\n",
       " 'between': 774,\n",
       " 'bewilder': 775,\n",
       " 'bewilderingly': 776,\n",
       " 'beyond': 777,\n",
       " 'bias': 778,\n",
       " 'bib': 779,\n",
       " 'bibbidy': 780,\n",
       " 'bible': 781,\n",
       " 'bicker': 782,\n",
       " 'bielinsky': 783,\n",
       " 'big': 784,\n",
       " 'bigger': 785,\n",
       " 'biggest': 786,\n",
       " 'bike': 787,\n",
       " 'bile': 788,\n",
       " 'bill': 789,\n",
       " 'binary': 790,\n",
       " 'bind': 791,\n",
       " 'bio': 792,\n",
       " 'biographical': 793,\n",
       " 'biography': 794,\n",
       " 'biologically': 795,\n",
       " 'biopic': 796,\n",
       " 'birmingham': 797,\n",
       " 'birth': 798,\n",
       " 'birthday': 799,\n",
       " 'biscuit': 800,\n",
       " 'bitch': 801,\n",
       " 'bite': 802,\n",
       " 'bites': 803,\n",
       " 'bits': 804,\n",
       " 'bitter': 805,\n",
       " 'bittersweet': 806,\n",
       " 'biz': 807,\n",
       " 'bizarre': 808,\n",
       " 'black': 809,\n",
       " 'blackout': 810,\n",
       " 'blade': 811,\n",
       " 'bladerunner': 812,\n",
       " 'blah': 813,\n",
       " 'blair': 814,\n",
       " 'blame': 815,\n",
       " 'blanche': 816,\n",
       " 'bland': 817,\n",
       " 'blank': 818,\n",
       " 'blasphemous': 819,\n",
       " 'blast': 820,\n",
       " 'blatant': 821,\n",
       " 'blaxploitation': 822,\n",
       " 'blaze': 823,\n",
       " 'blazing': 824,\n",
       " 'bleak': 825,\n",
       " 'bleed': 826,\n",
       " 'blend': 827,\n",
       " 'blender': 828,\n",
       " 'blessed': 829,\n",
       " 'blind': 830,\n",
       " 'blindly': 831,\n",
       " 'blindness': 832,\n",
       " 'blip': 833,\n",
       " 'bliss': 834,\n",
       " 'blisteringly': 835,\n",
       " 'bloat': 836,\n",
       " 'block': 837,\n",
       " 'blockage': 838,\n",
       " 'blockbuster': 839,\n",
       " 'blonde': 840,\n",
       " 'blood': 841,\n",
       " 'bloodbath': 842,\n",
       " 'bloodletting': 843,\n",
       " 'bloodsucker': 844,\n",
       " 'bloody': 845,\n",
       " 'blow': 846,\n",
       " 'blue': 847,\n",
       " 'bluescreen': 848,\n",
       " 'blunder': 849,\n",
       " 'blurry': 850,\n",
       " 'bmw': 851,\n",
       " 'bo': 852,\n",
       " 'board': 853,\n",
       " 'boarders': 854,\n",
       " 'boardwalk': 855,\n",
       " 'boast': 856,\n",
       " 'boasting': 857,\n",
       " 'boat': 858,\n",
       " 'boatload': 859,\n",
       " 'bobbidi': 860,\n",
       " 'bodies': 861,\n",
       " 'body': 862,\n",
       " 'bog': 863,\n",
       " 'bogdanovich': 864,\n",
       " 'bogged': 865,\n",
       " 'bogs': 866,\n",
       " 'bogus': 867,\n",
       " 'boil': 868,\n",
       " 'boisterous': 869,\n",
       " 'bold': 870,\n",
       " 'bolder': 871,\n",
       " 'boldly': 872,\n",
       " 'bollywood': 873,\n",
       " 'bolster': 874,\n",
       " 'bolstered': 875,\n",
       " 'bolt': 876,\n",
       " 'bomb': 877,\n",
       " 'bombastic': 878,\n",
       " 'bombshell': 879,\n",
       " 'bon': 880,\n",
       " 'bona': 881,\n",
       " 'bond': 882,\n",
       " 'bone': 883,\n",
       " 'bonus': 884,\n",
       " 'boo': 885,\n",
       " 'booby': 886,\n",
       " 'book': 887,\n",
       " 'boom': 888,\n",
       " 'boomer': 889,\n",
       " 'boorishness': 890,\n",
       " 'boost': 891,\n",
       " 'boot': 892,\n",
       " 'border': 893,\n",
       " 'bordering': 894,\n",
       " 'borderline': 895,\n",
       " 'bore': 896,\n",
       " 'boredom': 897,\n",
       " 'borg': 898,\n",
       " 'boring': 899,\n",
       " 'borrow': 900,\n",
       " 'boss': 901,\n",
       " 'botch': 902,\n",
       " 'both': 903,\n",
       " 'bother': 904,\n",
       " 'bottom': 905,\n",
       " 'bottomlessly': 906,\n",
       " 'bounce': 907,\n",
       " 'bound': 908,\n",
       " 'boundaries': 909,\n",
       " 'bounty': 910,\n",
       " 'bouquet': 911,\n",
       " 'bourgeois': 912,\n",
       " 'bout': 913,\n",
       " 'bow': 914,\n",
       " 'bowel': 915,\n",
       " 'bowl': 916,\n",
       " 'bowling': 917,\n",
       " 'box': 918,\n",
       " 'boxer': 919,\n",
       " 'boy': 920,\n",
       " 'boys': 921,\n",
       " 'brace': 922,\n",
       " 'bracing': 923,\n",
       " 'brain': 924,\n",
       " 'brainpower': 925,\n",
       " 'branch': 926,\n",
       " 'brand': 927,\n",
       " 'brash': 928,\n",
       " 'brass': 929,\n",
       " 'brave': 930,\n",
       " 'braveheart': 931,\n",
       " 'bravely': 932,\n",
       " 'bravery': 933,\n",
       " 'bravo': 934,\n",
       " 'brazil': 935,\n",
       " 'breach': 936,\n",
       " 'breaches': 937,\n",
       " 'bread': 938,\n",
       " 'breadth': 939,\n",
       " 'break': 940,\n",
       " 'breaks': 941,\n",
       " 'breakthrough': 942,\n",
       " 'breast': 943,\n",
       " 'breath': 944,\n",
       " 'breathe': 945,\n",
       " 'breathless': 946,\n",
       " 'breathtaking': 947,\n",
       " 'breed': 948,\n",
       " 'breeze': 949,\n",
       " 'breillat': 950,\n",
       " 'breitbart': 951,\n",
       " 'brennan': 952,\n",
       " 'brent': 953,\n",
       " 'brett': 954,\n",
       " 'brewery': 955,\n",
       " 'brian': 956,\n",
       " 'brick': 957,\n",
       " 'bride': 958,\n",
       " 'bridge': 959,\n",
       " 'bright': 960,\n",
       " 'brightest': 961,\n",
       " 'brightly': 962,\n",
       " 'brilliance': 963,\n",
       " 'brilliant': 964,\n",
       " 'brilliantly': 965,\n",
       " 'bring': 966,\n",
       " 'brings': 967,\n",
       " 'brisk': 968,\n",
       " 'brit': 969,\n",
       " 'british': 970,\n",
       " 'britney': 971,\n",
       " 'brittle': 972,\n",
       " 'broad': 973,\n",
       " 'broader': 974,\n",
       " 'broadway': 975,\n",
       " 'brockovich': 976,\n",
       " 'bromide': 977,\n",
       " 'bronx': 978,\n",
       " 'bronze': 979,\n",
       " 'brood': 980,\n",
       " 'brook': 981,\n",
       " 'brooklyn': 982,\n",
       " 'broomfield': 983,\n",
       " 'brother': 984,\n",
       " 'brothers': 985,\n",
       " 'brought': 986,\n",
       " 'brow': 987,\n",
       " 'brown': 988,\n",
       " 'brownish': 989,\n",
       " 'browser': 990,\n",
       " 'bruce': 991,\n",
       " 'bruckheimer': 992,\n",
       " 'bruise': 993,\n",
       " 'bruised': 994,\n",
       " 'brush': 995,\n",
       " 'brutal': 996,\n",
       " 'brutality': 997,\n",
       " 'brutally': 998,\n",
       " 'bryan': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#list of text document\n",
    "courpus = X.input_data1.values\n",
    "# create transorm\n",
    "vectorizer = TfidfVectorizer()\n",
    "# tokeniz and build vocab\n",
    "vectorizer.fit(courpus)\n",
    "#summarize\n",
    "# print(vectorizer.vocabulary_)\n",
    "\n",
    "dict(sorted(vectorizer.vocabulary_.items(), key=lambda item: item[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee74930",
   "metadata": {},
   "source": [
    "# Now transform Training and Testing input text into vector using above vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbf9bbd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training X: 13023 and Y: 13023\n",
      "Testing X: 3255 and Y: 3255\n"
     ]
    }
   ],
   "source": [
    "train_x = vectorizer.transform(train.input_data1.values).toarray()\n",
    "test_x = vectorizer.transform(test.input_data1.values).toarray()\n",
    "\n",
    "train_y  = train.Sentiment.values\n",
    "test_y = test.Sentiment.values\n",
    "\n",
    "print(f\"Training X: {len(train_x)} and Y: {len(train_y)}\")\n",
    "print(f\"Testing X: {len(test_x)} and Y: {len(test_y)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c672101f",
   "metadata": {},
   "source": [
    "# Multinomial Naive Bayes for Multi Class Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c82e4c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "mclf = MultinomialNB()\n",
    "\n",
    "mclf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff69e8b",
   "metadata": {},
   "source": [
    "# Now Apply prediction on Unseendata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "590d526d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict = mclf.predict(test_x)\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dd704e",
   "metadata": {},
   "source": [
    "# sklearn.metrics.classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0962e961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1241,  173],\n",
       "       [  79, 1762]], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "cm = confusion_matrix(test_y, y_predict)\n",
    "cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e275596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.88      0.91      1414\n",
      "           1       0.91      0.96      0.93      1841\n",
      "\n",
      "    accuracy                           0.92      3255\n",
      "   macro avg       0.93      0.92      0.92      3255\n",
      "weighted avg       0.92      0.92      0.92      3255\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#                            Actual, un-seen-data\n",
    "print(classification_report(test_y,y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "64175952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9225806451612903"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_y, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fbae42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "91bb753b057673435fb8d6f6a083e6c818364728098c7ae050ca3a25357dd754"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
